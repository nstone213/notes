Keras: the high level API for TensorFlow;
    Provides a highly productive interface for solving ML problems.
    Full access to the "scalability and cross-platform capabilities of TensorFlow.
    
Core data structures of Kera are <layers> and <models>.
    A layer is a simple input/output transformation, and a model is a directed, acyclic graph (DAG) of layers.

    Layers:
        import tensorflow as tf;
        tf.keras.layers.Layer -> class which is the fundamental abstraction in Keras. 
            A Layer encapsulates a state (weights) and some computation (Defined in the tf.keras.layers.Layer.call method);
            Weights created by layers can be trainable or non-trainable. Layers are recusively composable: If you assign a layer instance
                as an attribute of another layer, the outer layer will start tracking the weights created by the inner layer.
            You can also use layers to handle data preprocessing tasks like normalization and text vectorization. Preprocessing layers can be included
                directly into a model, either during or after training, which makes the model portable.
            
    Models:
        A model is an object that groups layers together and that can be trained on data.
        The simplest type of model is the <Sequential> model, which is a linear stack of layers. For more complex architectures, you can either use the 
            Keras functional API, which lets you build arbitrary graphs of layers, or use subclassing to write models from scratch.
        tf.keras.Model:
            tf.keras.Model.fit: trains the model for a fixed number of epochs.
            tf.keras.Model.predict: generates output predictions for the input samples.
            tf.keras.Model.evaluate: returns the loss and metrics values for the model; configured via the tf.keras.Model.compile method.

The Sequential Model
    Setup:
        import tensorflow as tf
        import keras
        from keras import layers
    
    When to use a Sequential Model:
        A Sequential model is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor.
    
    A Sequential model is not appropriate when:
        Your model has multiple inputs or multiple outputs.
        Any of your layers has multiple inputs or multiple outputs.
        You need to do layer sharing.
        You want non-linear topology (i.e. a residual connection, a multi-branch model)

        .Dense:
            keras.layers.Dense(units, activation=None, ...);
                Units: the number of neurons or units in the layer. This determines the dimensionality of the output space.
                Activation: the activation function to apply to the output of the layer (i.e. relu, sigmoid, softmax). If no activation is specified,
                    it defaults to a linear activation (f(x) = x).
                    -> Activation functions are mathematical functions applied to the output of neurons in a neural network layer. They introduce
                        non-linearity into the model, allowing it to learn complex patterns and make accurate predictions.
                        ReLU (Rectified Linear Unit):
                            Returns 0 if the input is negative and returns the input itself if it's positive.
                            f(x) = max(0,x)
                            Helps avoid the vanishing gradient problem (A challenge that occurs when training deep neural networks using gradient-based learning methods) by maintaining a linear response for positive inputs.
                            Can lead to "dead neurons" (neuron that stop learning) if many neurons output 0.
                            Common in hidden layers of deep networks, especially convolutional networks (CNNs).
                        Sigmoid (Logistic):
                            f(x) = 1/(1+e^-x)
                            Squashes input values into a range between 0 & 1.
                            Useful for binary classification, as it outputs values that can be interpreted as probabilities.
                            Can cause vanishing gradient problems during backpropagations in deep networks.
                            Gradient becomes very small for large or small inputs.
                            Often used in the output layer of binary classification models.
                        Tanh (Hyperbolic Tangent)
                            f(x) = tanh(x) = 2/(1+e^-2x) - 1;
                            Same behaviour as Sigmoid, but centers outout values 0, between -1 and 1.
                            Reduces bias because the output is zero-centered
                                -> Less prone to the vanishing gradient problem compared to the sigmoid function.
                            Still can suffer from vanishing gradients in deep networks.
                            Often used in hidden layers, especially in tasks requiring zero-centered outputs (like RNNs)
                        Softmax
                            Converts raw scores (logits) into probabilities by normalizing the inputs into a probability distribution where the sum of all outputs equals 1.
                            Useful for multi-class classification where the model needs to choose between multiple classes
                            Can be computationally intensive for large output spaces.
                            Typically used in the output layer of classification problems with more than two classes (such as image classification)
                        Leaky ReLU
                            f(x) = max(0.01x, x)
                            Similary to ReLU, but instead of outputting zero, it outputs a small, non-zero values
                            Solves the "dead neuron" problem in standard ReLU
                            Still retains the behaviour of a linear function for positive inputs, so not as complex as some other functions
                            Often used as an alternative to ReLU to improve gradient flow for negative inputs.
                Other parameters: You can specify other arguments like kernel initializer, bias initializer, regularization, etc., but units and activation
                    are the most important for most use cases.
                Note that there's also a corresponding pop() method to remove layers: a sequential model behaves very much like a list of layers (a stack)
                    model.pop()
                    print(len(model.layers)) //2
                The Sequential constructor also accepts a name argument, just like any layer or model in keras. This is useful to annotate TensorBoard graphs
                    with semantically meaningful names.
                    < Example
                        model = keras.Sequential(name="my_sequential")
                        model.add(layers.Dense(2, activation="relu", name="layer1"))
                        model.add(layers.Dense(3, activation="relu", name="layer2"))
                        model.add(layers.Dense(4, name="layer3"))
                    End Example >